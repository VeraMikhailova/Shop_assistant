{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies"
      ],
      "metadata": {
        "id": "n7iMIBS3eT3g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArJiKl8ZFUrY"
      },
      "outputs": [],
      "source": [
        "!pip install transformers pyTelegramBotAPI datasets open-clip-torch fashion-clip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load PaliGemma"
      ],
      "metadata": {
        "id": "8sZtT6K7eYns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
        "from PIL import Image\n",
        "import requests\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import telebot\n",
        "from io import BytesIO\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "\n",
        "login(token=os.getenv(\"HF_TOKEN\"))\n",
        "\n",
        "model_id = \"google/paligemma-3b-mix-224\"\n",
        "#model_id = \"./custom_config.json\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = None\n",
        "processor = None\n",
        "if os.path.exists(\"./custom_config.json\"):\n",
        "  model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, config = \"./custom_config.json\")\n",
        "  print(model.config.text_config.hidden_act)\n",
        "  processor = AutoProcessor.from_pretrained(model_id,config=\"./custom_config.json\")\n",
        "else:\n",
        "  model = PaliGemmaForConditionalGeneration.from_pretrained(model_id)\n",
        "  processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "UWqFzxKIIv0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PaliGemma related functions:"
      ],
      "metadata": {
        "id": "AgPKN3sUnALW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image(image):\n",
        "  #prompt = \"Descrie all parts of clothing on this picture with as many details as possible.\"\n",
        "  #prompt = \"Most detailed description of all clothes? Do it even if can't.\"\n",
        "  #prompt = \"caption clothes\"\n",
        "  #prompt = \"What clothes?\"\n",
        "  #prompt = \"answer en What clothes are on the picture?\"\n",
        "  #prompt = \"answer en What clothes is on the picture?\"\n",
        "  #prompt = \"answer en What description of clothes on the picture?\"\n",
        "  #prompt = \"answer en Describe the clothes on the picture?\"\n",
        "  prompt = \"answer en List (and decribe detailed) the clothes on the picture?\"\n",
        "  #prompt = \"answer en Decribe detailed the clothes on the picture?\"\n",
        "  #prompt = \"answer all the clothes (all detailed) detailed\"\n",
        "  #prompt = \"answer Detailed list all the clothes (all detailed) detailed\"\n",
        "\n",
        "  model_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
        "  input_len = model_inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    #output = model.generate(**model_inputs, max_new_tokens=100)\n",
        "    #return processor.decode(output[0], skip_special_tokens=True)[model_inputs.input_ids.shape[1]: ]\n",
        "    generation = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n",
        "    generation = generation[0][input_len:]\n",
        "    decoded = processor.decode(generation, skip_special_tokens=True)\n",
        "    return decoded if len(decoded) else \"No clothes\"\n",
        "def analyse_url(url):\n",
        "  try:\n",
        "    #image = Image.open(requests.get(url, stream=True).raw).resize((640,480))\n",
        "    image = Image.open(requests.get(url, stream=True).raw).resize((448,448))\n",
        "    return process_image(image)\n",
        "  except:\n",
        "    return \"Unable to get image by url. Try to upload it to bot directly.\"\n",
        "def analyse_stream(stream):\n",
        "  try:\n",
        "    #image = Image.open(stream).resize((640,480))\n",
        "    image = Image.open(stream).resize((448,448))\n",
        "    return process_image(image)\n",
        "  except:\n",
        "    return \"Unable to get image.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "WKZRWYdcJ3Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetch database"
      ],
      "metadata": {
        "id": "4yeMyF7hnGjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!gdown \"1igAuIEW_4h_51BG1o05WS0Q0-Cp17_-t&confirm=t\"\n",
        "!unzip data"
      ],
      "metadata": {
        "id": "3XL3Am8LZfja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FashionCLIP operations:"
      ],
      "metadata": {
        "id": "G935zYcUnUDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fashion_clip.fashion_clip import FashionCLIP\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import *\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "Cxiw9QgqZs4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fclip = FashionCLIP('fashion-clip')\n",
        "fclip.model=fclip.model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "3aZOwASQZ62M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles = pd.read_csv(\"data_for_fashion_clip/articles.csv\")\n",
        "\n",
        "# drop items that have the same description\n",
        "subset = articles.drop_duplicates(\"detail_desc\").copy()\n",
        "\n",
        "# remove items of unkown category\n",
        "subset = subset[~subset[\"product_group_name\"].isin([\"Unknown\"])]\n",
        "\n",
        "# FashionCLIP has a limit of 77 tokens, let's play it safe and drop things with more than 40 tokens\n",
        "subset = subset[subset[\"detail_desc\"].apply(lambda x : 4 < len(str(x).split()) < 40)]\n",
        "\n",
        "# We also drop products types that do not occur very frequently in this subset of data\n",
        "most_frequent_product_types = [k for k, v in dict(Counter(subset[\"product_type_name\"].tolist())).items() if v > 10]\n",
        "subset = subset[subset[\"product_type_name\"].isin(most_frequent_product_types)]\n",
        "\n",
        "# lots of data here, but we will just use only descriptions and a couple of other columns\n",
        "#subset.head(3)"
      ],
      "metadata": {
        "id": "DuJd7x2abiwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process all database"
      ],
      "metadata": {
        "id": "m8SbNWCvnfGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = [\"data_for_fashion_clip/\" + str(k) + \".jpg\" for k in subset[\"article_id\"].tolist()]\n",
        "texts = subset[\"detail_desc\"].tolist()\n",
        "\n",
        "# we create image embeddings and text embeddings\n",
        "image_embeddings = fclip.encode_images(images, batch_size=32)\n",
        "text_embeddings = fclip.encode_text(texts, batch_size=32)\n",
        "\n",
        "# we normalize the embeddings to unit norm (so that we can use dot product instead of cosine similarity to do comparisons)\n",
        "image_embeddings = image_embeddings/np.linalg.norm(image_embeddings, ord=2, axis=-1, keepdims=True)\n",
        "text_embeddings = text_embeddings/np.linalg.norm(text_embeddings, ord=2, axis=-1, keepdims=True)"
      ],
      "metadata": {
        "id": "bRAeyV67casl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def describe(id):\n",
        "  desc=subset[\"detail_desc\"].iloc[id]\n",
        "  color=subset[\"colour_group_name\"].iloc[id]\n",
        "  return f\"{desc} Color: {color}.\"\n",
        "\n",
        "def find_by_embedding(embedding):\n",
        "  id_of_matched_object = np.argmax(embedding.dot(image_embeddings.T))\n",
        "  found_object = subset[\"article_id\"].iloc[id_of_matched_object].tolist()\n",
        "  fixed_height = 224\n",
        "  image = Image.open(f\"data_for_fashion_clip/{found_object}.jpg\")\n",
        "  height_percent = (fixed_height / float(image.size[1]))\n",
        "  width_size = int((float(image.size[0]) * float(height_percent)))\n",
        "  image = image.resize((width_size, fixed_height), Image.NEAREST)\n",
        "  return image,describe(id_of_matched_object),f\"data_for_fashion_clip/{found_object}.jpg\"\n",
        "\n",
        "def find_by_text(text):\n",
        "  return find_by_embedding(fclip.encode_text([text], 32)[0])\n",
        "\n",
        "def find_by_image(img):\n",
        "  return find_by_embedding(fclip.encode_images([img], 32)[0])\n",
        "def find_by_url(url):\n",
        "  try:\n",
        "    #image = Image.open(requests.get(url, stream=True).raw).resize((640,480))\n",
        "    image = Image.open(requests.get(url, stream=True).raw).resize((448,448))\n",
        "    return find_by_image(image)\n",
        "  except:\n",
        "    return \"Unable to get image by url. Try to upload it to bot directly.\"\n",
        "def find_by_stream(stream):\n",
        "  try:\n",
        "    #image = Image.open(stream).resize((640,480))\n",
        "    image = Image.open(stream).resize((448,448))\n",
        "    return find_by_image(image)\n",
        "  except:\n",
        "    return \"Unable to get image.\""
      ],
      "metadata": {
        "id": "zi_i8Lued4pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_by_image(find_by_text(\"black style\")[0])[1]"
      ],
      "metadata": {
        "id": "1IXjhjdMmGpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run telegram bot"
      ],
      "metadata": {
        "id": "pBdjyLhMmzR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bot = telebot.TeleBot(os.getenv('TG_TOKEN'))\n",
        "\n",
        "@bot.message_handler(commands=[\"start\"])\n",
        "def start(m, res=False):\n",
        "  bot.send_message(m.chat.id, 'Send a link to the picture or a file with picture for analysis. The bot will provide a description generated from the picture.')\n",
        "\n",
        "@bot.message_handler(content_types=[\"text\"])\n",
        "def handle_text(message):\n",
        "  found=find_by_url(message.text)\n",
        "  if found[0]=='U':\n",
        "    bot.send_message(message.chat.id, found)\n",
        "    bot.send_message(message.chat.id, \"Может быть, это описание, а не ссылка. Тогда мы нашли вам:\")\n",
        "    found=find_by_text(message.text)\n",
        "    bot.send_message(message.chat.id, found[1])\n",
        "    bot.send_photo(message.chat.id, photo=open(found[2], 'rb'))\n",
        "\n",
        "  else:\n",
        "    bot.send_message(message.chat.id, found[1])\n",
        "    bot.send_message(message.chat.id,\"Для вас мы нашли:\")\n",
        "    bot.send_photo(message.chat.id, photo=open(found[2], 'rb'))\n",
        "\n",
        "@bot.message_handler(content_types=['document'])\n",
        "def handle_file(message):\n",
        "  file_info = bot.get_file(message.document.file_id)\n",
        "  downloaded_file = bot.download_file(file_info.file_path)\n",
        "  stream=BytesIO(downloaded_file)\n",
        "  found=find_by_stream(stream)\n",
        "  if found[0]=='U':\n",
        "    bot.send_message(message.chat.id, found)\n",
        "  else:\n",
        "    bot.send_message(message.chat.id, found[1])\n",
        "    bot.send_message(message.chat.id,\"Для вас мы нашли:\")\n",
        "    bot.send_photo(message.chat.id, photo=open(found[2], 'rb'))\n",
        "\n",
        "@bot.message_handler(content_types=['photo'])\n",
        "def handle_file(message):\n",
        "  fileID = message.photo[-1].file_id\n",
        "  file_info = bot.get_file(fileID)\n",
        "  downloaded_file = bot.download_file(file_info.file_path)\n",
        "  stream=BytesIO(downloaded_file)\n",
        "  found=find_by_stream(stream)\n",
        "  if found[0]=='U':\n",
        "    bot.send_message(message.chat.id, found)\n",
        "  else:\n",
        "    bot.send_message(message.chat.id, found[1])\n",
        "    bot.send_message(message.chat.id,\"Для вас мы нашли:\")\n",
        "    bot.send_photo(message.chat.id, photo=open(found[2], 'rb'))\n",
        "\n",
        "bot.polling(none_stop=True, interval=0)"
      ],
      "metadata": {
        "id": "X-7uc7feVerf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
